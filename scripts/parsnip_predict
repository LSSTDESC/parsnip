#!/usr/bin/env python
import os
import sys
import parsnip
import argparse
import time
import pandas as pd
from tqdm import tqdm


def parse_int_list(text):
    result = [int(i) for i in text.split(',')]
    return result


if __name__ == '__main__':
    start_time = time.time()

    parser = argparse.ArgumentParser(description='Apply a parsnip model to a dataset',
                                     argument_default=argparse.SUPPRESS)
    parser.add_argument('model')
    parser.add_argument('dataset')

    parser.add_argument('--chunk_size', default=10000, type=int)
    parser.add_argument('--augments', default=0, type=int)

    parser.add_argument('--device', default='cuda')
    parser.add_argument('--threads', default=8, type=int)

    args = vars(parser.parse_args())

    # Load the model
    model = parsnip.load_model(
        args['model'],
        device=args['device'],
        threads=args['threads'],
    )

    # Load the metadata for the dataset
    meta_dataset = parsnip.load_dataset(args['dataset'], metadata_only=True)

    # Parse the dataset in chunks. For large datasets, we can't fit them all in memory
    # at the same time.
    chunk_size = args['chunk_size']
    num_chunks = len(meta_dataset) // chunk_size + 1

    # Optionally, the dataset can be augmented a given number of times.
    augments = args['augments']

    predictions = []

    for chunk in tqdm(range(num_chunks), file=sys.stdout):
        # Load a chunk of the dataset
        dataset = parsnip.load_dataset(args['dataset'], num_chunks=num_chunks,
                                       chunk=chunk, verbose=False)

        # Preprocess the light curves
        model.preprocess(dataset, verbose=False)

        # Generate the prediction
        if augments == 0:
            chunk_predictions = model.predict_dataset(dataset)
        else:
            chunk_predictions = model.predict_dataset_augmented(dataset,
                                                                augments=augments)
        predictions.append(chunk_predictions)

    # Save the predictions
    predictions = pd.concat(predictions)
    os.makedirs('./predictions', exist_ok=True)
    if augments > 0:
        output_name = f"{args['dataset']}_{args['model']}_aug_{augments}"
    else:
        output_name = f"{args['dataset']}_{args['model']}"

    predictions.to_hdf(f'./predictions/{output_name}.h5', 'predictions', 'w')

    # Calculate time taken in minutes
    end_time = time.time()
    elapsed_time = (end_time - start_time) / 60.
    print(f"Total time: {elapsed_time:.2f} min")
